{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "930cb9b3-1016-4140-92f8-8e9510bf99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/PC-EIAD209/Desktop/AnaKei/NIPD-AI\")\n",
    "#sys.path.append(\"C:\\\\Users\\\\\"+os.getlogin()+\"\\\\OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey\\\\PainClassifier\")\n",
    "from my_data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b514f89d-e9b9-4ecb-afa9-84963e04971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.utils import resample\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv3D, MaxPooling3D, Flatten, Dropout, GlobalAveragePooling3D, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "\n",
    "#from keras.regularizers import l2\n",
    "import cv2\n",
    "#from keras import initializers\n",
    "#from keras.layers import LeakyReLU\n",
    "#from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "#from keras import losses\n",
    "\n",
    "#nuevo\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import initializers, losses\n",
    "from tensorflow.keras.layers import LeakyReLU, BatchNormalization\n",
    "\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import gc\n",
    "#from numba import cuda   nota: no se usa\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "import nibabel as nib\n",
    "#from skiimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad5cd65c-6ba9-4dae-b349-b042293143da",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55689aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rabies_ref shape: (48, 81, 48)\n"
     ]
    }
   ],
   "source": [
    "rabies_ref_path= r\"F:\\New data\\sigma_files\\sigma_files\\SIGMA_resam_InVivo_Brain_Template_Masked.nii\"\n",
    "rabies_ref = nib.load(rabies_ref_path).get_fdata()\n",
    "#rabies_vol= np.mean(rabies_ref, axis=3)\n",
    "print(\"rabies_ref shape:\", rabies_ref.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76918608",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mover a my data generator \n",
    "def filter_heatmap(heatmap, threshold=0.7):\n",
    "    heatmap = heatmap.astype(np.float32)\n",
    "    if np.max(heatmap) > 0:\n",
    "        heatmap /= np.max(heatmap)  #normalizar entre 0 y 1\n",
    "    heatmap[heatmap < threshold] = 0\n",
    "    return heatmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "93afc1b9-732d-4691-bdee-14840855df92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed precision training\n",
    "\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('float32')\n",
    "\n",
    "#DirectML no se lleva com mixedfloat16\n",
    "#mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1dc7abe-1101-4633-9fe8-7200ae647614",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_3D(blocks):\n",
    "        \n",
    "    inputs = Input(shape=(42, 65, 29), name='input_layer')\n",
    "    x = Reshape(target_shape=[42, 65, 29, 1], name='input_x_3d_volumes')(inputs)\n",
    "\n",
    "    if blocks == 1:\n",
    "        print(\"entra al 1\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 2:\n",
    "        print(\"entra al 2\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 3:\n",
    "        print(\"entra al 3\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 4:\n",
    "        print(\"entra al 4\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        #x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    elif blocks == 5:\n",
    "        print(\"entra al 5\")\n",
    "        #batch_norm\n",
    "        #x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        #x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        #x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x) \n",
    "        x = BatchNormalization()(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        #x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    \n",
    "        #batch_norm\n",
    "        #x = BatchNormalization()(x)\n",
    "        # 5th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.001))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.1)(x)\n",
    "    \n",
    "    # Fully connected layers  \n",
    "    x = Flatten()(x) \n",
    "    x = Dense(units = 1024, activation ='relu',kernel_regularizer='l2')(x)\n",
    "    #x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = Dense(units = 1024, activation ='relu',kernel_regularizer='l2')(x) \n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    output = Dense(units = 2,activation ='softmax',kernel_regularizer='l2')(x)\n",
    "    \n",
    "    # creating the model\n",
    "    VGG_3d_model = Model (inputs=inputs, outputs =output)\n",
    "    #model.summary()\n",
    "\n",
    "    return VGG_3d_model\n",
    "\n",
    "def set_pretrained_weigths(VGG_3d_model):\n",
    "    #VGG 16 with weights from Imagenet\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling='avg',\n",
    "        input_shape = (42, 65, 3)\n",
    "    )\n",
    "    \n",
    "    #conv layers on VGG_3d_model\n",
    "    layers_conv = []\n",
    "    for j in range(len(VGG_3d_model.layers)):\n",
    "        if \"conv3d\" in str(VGG_3d_model.layers[j]):\n",
    "            layers_conv.append(j)\n",
    "    layers_conv_pretrained = []\n",
    "    for j in range(len(pretrained_model.layers)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[j]):\n",
    "            layers_conv_pretrained.append(j)\n",
    "    \n",
    "    for i in range(len(layers_conv)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[layers_conv_pretrained[i]]):\n",
    "            if i == 0:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0].sum(axis=2, keepdims=True)\n",
    "            else:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0]\n",
    "                \n",
    "            w3d=[]\n",
    "            \n",
    "            w = np.reshape(w,(3,3,-1),order='F')\n",
    "            for j in range(len(w[0,0,:])):\n",
    "                for k in range(3):\n",
    "                    w3d.append(w[:,:,j])\n",
    "            w3d = np.transpose(w3d, (1,2,0))\n",
    "            \n",
    "            new_weights = np.reshape(w3d, np.array(VGG_3d_model.layers[layers_conv[i]].get_weights()[0]).shape,order='F')\n",
    "            new_bias = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[1]\n",
    "            \n",
    "            WnB = []\n",
    "            WnB.append(new_weights)\n",
    "            WnB.append(new_bias)\n",
    "    \n",
    "            VGG_3d_model.layers[layers_conv[i]].set_weights(WnB)\n",
    "\n",
    "    del pretrained_model, w, WnB, new_weights, new_bias, w3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf8e2858-67dd-4134-a5a1-63366a491da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionmatrix_multiclass(y_test,pred):\n",
    "    cm = confusion_matrix(y_test, (np.rint(preds)).astype(int) )\n",
    "    group_names = ['True baseline','False Baseline','False Baseline',   \n",
    "                   'False week 1','Truec','False Week 1',\n",
    "                  'False week 7','False week 7','True week 7']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(3,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(3,3)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['Baseline','Week 1','Week 7'] ,yticklabels = ['Baseline','Week 1','Week 7'])\n",
    "    plt.show()\n",
    "\n",
    "def confusionmatrix(y_test,preds):\n",
    "    #Construct the Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True Male','False Male','False Female','True Female']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['MALE','FEMALE'] ,yticklabels = ['MALE','FEMALE'])\n",
    "    plt.show()\n",
    "    return sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['MALE','FEMALE'] ,yticklabels = ['MALE','FEMALE'])\n",
    "    \n",
    "def confusionmatrix_binary(y_test, preds):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True baseline','False baseline','False Week 1','True Week 1']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['MALE','FEMALE'] ,yticklabels = ['MALE','FEMALE'])\n",
    "    plt.show()\n",
    "\n",
    "def ROC(probs,y_test): #binary\n",
    "    #Classification Area under curve\n",
    "     warnings.filterwarnings('ignore')\n",
    "             \n",
    "     auc = roc_auc_score(y_test, probs)\n",
    "     print('AUC - Test Set: %.2f%%' % (auc*100))\n",
    "    \n",
    "     # calculate roc curve\n",
    "     fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "     # plot no skill\n",
    "     plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "     # plot the roc curve for the model\n",
    "     plt.plot(fpr, tpr, marker='.')\n",
    "     plt.xlabel('False positive rate')\n",
    "     plt.ylabel('Sensitivity/ Recall')\n",
    "     # show the plot\n",
    "     plt.show()\n",
    "    \n",
    "     probs = (np.rint(probs)).astype(int)   \n",
    "        \n",
    "     precision = precision_score(y_test, probs)\n",
    "     print('Precision: %f' % precision)\n",
    "     # recall: tp / (tp + fn)\n",
    "     recall = recall_score(y_test, probs)\n",
    "     print('Recall: %f' % recall)\n",
    "     # f1: tp / (tp + fp + fn)\n",
    "     f1 = f1_score(y_test, probs)\n",
    "     print('F1 score: %f' % f1)\n",
    "        \n",
    "def ROC_multiclass(model, y_test, n_class):\n",
    "    #y_test: array size (# of subjects, ) with classes \n",
    "    #pretrained model to be evaluated \n",
    "    \n",
    "    label_binarizer = LabelBinarizer().fit(y_test)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    y_score = model.predict(X_test) # y_score is onehot\n",
    "    \n",
    "    # store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")\n",
    "    \n",
    "    n_classes = n_class\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = fpr_grid\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")\n",
    "    \n",
    "    target_names = ['Naive','Week1','Week7']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"ROC curve for {target_names[class_id]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "            plot_chance_level=(class_id == 2),\n",
    "        )\n",
    "\n",
    "    _ = ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\",\n",
    "    )\n",
    "    \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "    c = ['b','g','r','c','m','y','k','w']\n",
    "    ltr = ['fold 1(train)','fold 2(train)','fold 3(train)','fold 4(train)','fold 5(train)']\n",
    "    lts = ['fold 1(val)','fold 2(val)','fold 3(val)','fold 4(val)','fold 5(val)']\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(histories[i].history['loss'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_loss'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "        # plot accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(histories[i].history['Accuracy'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_Accuracy'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    plt.boxplot(scores)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710dacbe-c461-4d1d-be96-75c0a184f29e",
   "metadata": {},
   "source": [
    "# Just Brain Male-BL vs Female-BL "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e33440f-8150-4310-b0c1-fe68db52d0e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entra al 5\n",
      "entra al 5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "male = [57, \n",
    "        #59,\n",
    "        60,73,74,93,94,95,96,98,99,100]\n",
    "female = [49,50,51,52,65,66,77,78,79,80,81,82,\n",
    "          #83\n",
    "         ]\n",
    "         \n",
    "#male = [57,60,73,74,93,]\n",
    "#female = [49,50,51,52]\n",
    "\n",
    "y_male = np.ones(len(male))\n",
    "y_female = np.zeros(len(female))\n",
    "\n",
    "subjects = np.array(male+female)\n",
    "labels = np.array(list(y_male)+list(y_female))\n",
    "sessions = [1]\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"dist\"\n",
    "\n",
    "#Pretrained model to re start weights from at every fold\n",
    "baseModel = VGG16_3D(5)\n",
    "set_pretrained_weigths(baseModel)\n",
    "\n",
    "#Model that is going to be train and re started every fold\n",
    "CNN = VGG16_3D(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14529fcb-5dff-4412-8f0b-7b1e37522403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Bootstrapping Run 1/1 ===\n",
      "Run # 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: a01067716 (a01067716-tecnol-gico-de-monterrey) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\PC-EIAD209\\Desktop\\AnaKei\\NIPD-AI\\wandb\\run-20251003_115305-grxs6tlp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/a01067716-tecnol-gico-de-monterrey/Male%20vs%20Female%28CPH%29/runs/grxs6tlp' target=\"_blank\">bright-dragon-26</a></strong> to <a href='https://wandb.ai/a01067716-tecnol-gico-de-monterrey/Male%20vs%20Female%28CPH%29' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/a01067716-tecnol-gico-de-monterrey/Male%20vs%20Female%28CPH%29' target=\"_blank\">https://wandb.ai/a01067716-tecnol-gico-de-monterrey/Male%20vs%20Female%28CPH%29</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/a01067716-tecnol-gico-de-monterrey/Male%20vs%20Female%28CPH%29/runs/grxs6tlp' target=\"_blank\">https://wandb.ai/a01067716-tecnol-gico-de-monterrey/Male%20vs%20Female%28CPH%29/runs/grxs6tlp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub train:\n",
      "['F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id100.session01_split_name_sub-100_ses-01_desc-o_T2w/_run_None/sub-100_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id081.session01_split_name_sub-081_ses-01_desc-o_T2w/_run_None/sub-081_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-001/commonspace_bold/_scan_info_subject_id057.session01_split_name_sub-057_ses-01_desc-o_T2w/_run_None/sub-057_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id095.session01_split_name_sub-095_ses-01_desc-o_T2w/_run_None/sub-095_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id079.session01_split_name_sub-079_ses-01_desc-o_T2w/_run_None/sub-079_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id080.session01_split_name_sub-080_ses-01_desc-o_T2w/_run_None/sub-080_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id095.session01_split_name_sub-095_ses-01_desc-o_T2w/_run_None/sub-095_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz']\n",
      "sub test:\n",
      "['F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id100.session01_split_name_sub-100_ses-01_desc-o_T2w/_run_None/sub-100_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id073.session01_split_name_sub-073_ses-01_desc-o_T2w/_run_None/sub-073_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-001/commonspace_bold/_scan_info_subject_id060.session01_split_name_sub-060_ses-01_desc-o_T2w/_run_None/sub-060_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-001/commonspace_bold/_scan_info_subject_id049.session01_split_name_sub-049_ses-01_desc-o_T2w/_run_None/sub-049_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id078.session01_split_name_sub-078_ses-01_desc-o_T2w/_run_None/sub-078_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz']\n",
      "sub val:\n",
      "['F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id080.session01_split_name_sub-080_ses-01_desc-o_T2w/_run_None/sub-080_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz'\n",
      " 'F:/rabies/preprocess_batch-002/commonspace_bold/_scan_info_subject_id096.session01_split_name_sub-096_ses-01_desc-o_T2w/_run_None/sub-096_ses-01_task-dist_desc-oa_bold_autobox_combined.nii.gz']\n",
      "# sesiones Train 7\n",
      "# sesiones Test 5\n",
      "# sesiones Val 2\n",
      "Starting VGG 16 3D-----------------------------------------------------\n",
      "Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING WandbCallback is deprecated and will be removed in a future release. Please use the WandbMetricsLogger, WandbModelCheckpoint, and WandbEvalCallback callbacks instead. See https://docs.wandb.ai/guides/integrations/keras for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1/7 [===>..........................] - ETA: 5:12 - loss: 22.6677 - Accuracy: 0.4333"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 125\u001b[0m\n\u001b[0;32m    121\u001b[0m start_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 125\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mCNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraingen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mWandbCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msave_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    128\u001b[0m end_time \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDuration (CNN): \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(end_time \u001b[38;5;241m-\u001b[39m start_time))\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:171\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cbk \u001b[38;5;129;01min\u001b[39;00m cbks:\n\u001b[0;32m    170\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m old_v2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:171\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cbk \u001b[38;5;129;01min\u001b[39;00m cbks:\n\u001b[0;32m    170\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m old_v2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\wandb\\integration\\keras\\keras.py:171\u001b[0m, in \u001b[0;36mpatch_tf_keras.<locals>.new_v2\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cbk \u001b[38;5;129;01min\u001b[39;00m cbks:\n\u001b[0;32m    170\u001b[0m         set_wandb_attrs(cbk, val_data)\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m old_v2(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\PC-EIAD209\\.conda\\envs\\nipd\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    " # 80% train y val y 20% test\n",
    "\n",
    "n_bootstraps= 1\n",
    "for boot in range(n_bootstraps): \n",
    "    print(f\"\\n=== Bootstrapping Run {boot+1}/{n_bootstraps} ===\")\n",
    "\n",
    "    # Resample subjects + labels con reemplazo\n",
    "    boot_subjects, boot_labels = resample(\n",
    "     subjects, labels, replace=True, random_state=42+boot\n",
    "    )\n",
    "\n",
    "    sub_trainval, sub_test, y_trainval, y_test = train_test_split(\n",
    "    boot_subjects, boot_labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "    scores, histories = list(), list()\n",
    "    run = 1\n",
    "    for train_ix, val_ix in kfold.split(sub_trainval, y_trainval):\n",
    "        print(\"Run #\",run)\n",
    "        sub_train, sub_val = boot_subjects[train_ix], boot_subjects[val_ix]\n",
    "        y_train, y_val     = boot_labels[train_ix], boot_labels[val_ix]\n",
    "\n",
    "        sub_test, y_test = sub_test, y_test\n",
    "\n",
    "        \n",
    "\n",
    "        # Set default values\n",
    "        config_defaults = {\n",
    "            \"batch\": 2,\n",
    "        }\n",
    "        # Initialize wandb with a sample project name\n",
    "        wandb.init(project=\"Male vs Female(CPH)\", notes=\"FOLD \"+str(run)+\".  Male-W7 vs Female-W7. Rest-approach on DIST data. GPT recommendations. 15 epoch. Just z-scoring and applying RandomFlip. All layers gradcam. pooling just on first block. lr = 1e-6\",\n",
    "                config=config_defaults)\n",
    "\n",
    "        # Specify the other hyperparameters to the configuration.\n",
    "        wandb.config.epochs = 2 #antes 15\n",
    "        wandb.config.sub_batch = 30\n",
    "        wandb.config.sub_batch_ts = 30\n",
    "        wandb.config.subjects = subjects\n",
    "        wandb.config.architecture_name = \"VGG16_3D\"\n",
    "        wandb.config.dataset_name = \"Male vs Female(CPH[w7])-Rest-approach_DIST-data\"\n",
    "        wandb.config.CNN_blocks = 5\n",
    "        wandb.config.sessions = sessions\n",
    "        wandb.config.vols_per_session_tr = 30 #570\n",
    "        wandb.config.vols_per_session_ts = 30 #570\n",
    "        wandb.config.initial_learning_rate = 1e-5\n",
    "        #wandb.config.lr_decay_rate = 0.95\n",
    "        wandb.config.optimizer = \"Adam\"\n",
    "        \n",
    "        #\n",
    "        val_size = max(2, int(round(0.1 * len(train_ix))))\n",
    "    \n",
    "        sub_train, sub_val, y_train, y_val = train_test_split(\n",
    "            sub_trainval[train_ix], y_trainval[train_ix],\n",
    "            test_size=val_size,  # test_size se refiere a la validacion\n",
    "            random_state=42, stratify=y_trainval[train_ix]\n",
    "        )\n",
    "\n",
    "        sub_test, y_test = sub_test, y_test\n",
    "        #\n",
    "        \n",
    "        CPHclassTrain = FILES_and_LABELS(sub_train, sessions, MRI_type, functional_type)\n",
    "        CPHclassTest = FILES_and_LABELS(sub_test, sessions, MRI_type, functional_type)\n",
    "        CPHclassval = FILES_and_LABELS(sub_val, sessions, MRI_type, functional_type)\n",
    "            \n",
    "        X_train = CPHclassTrain.get_mask_and_bold()\n",
    "        X_test = CPHclassTest.get_mask_and_bold()\n",
    "        X_val = CPHclassval.get_mask_and_bold()\n",
    "\n",
    "        print(\"sub train:\")\n",
    "        print(np.array(X_train)[:,0])\n",
    "        print(\"sub test:\")\n",
    "        print(np.array(X_test)[:,0])\n",
    "        print(\"sub val:\")\n",
    "        print(np.array(X_val)[:,0])\n",
    "\n",
    "        print(\"# sesiones Train\",len(X_train))\n",
    "        print(\"# sesiones Test\",len(X_test))\n",
    "        print(\"# sesiones Val\",len(X_val))\n",
    "        \n",
    "        traingen = CustomDataGen(X_train, batch_size=wandb.config.batch, subbatch_size=wandb.config.sub_batch,\n",
    "                                format = \"just_brain\", vols = wandb.config.vols_per_session_tr,\n",
    "                                num_class = 2, classes = \"sex\", augmentation = True, functional_type = \"rest\")\n",
    "        traingen.on_epoch_end()\n",
    "        #Es necesario que la division entre X_test y batch_size tenga un modulo igual a 0. \n",
    "        #De otra manera el ultimo batch no lo utiliza al utilizar .predict\n",
    "        testgen  = CustomDataGen(X_test, batch_size=1,subbatch_size=wandb.config.sub_batch_ts,\n",
    "                                format = \"just_brain\", vols= wandb.config.vols_per_session_ts,\n",
    "                                num_class = 2, classes = \"sex\",shuffle=False, functional_type = \"rest\")\n",
    "        valgen  = CustomDataGen(X_val, batch_size=len(X_val),subbatch_size=30, format = \"just_brain\",vols=570, augmentation = True, num_class = 2,\n",
    "                                classes = \"sex\", functional_type = \"rest\")\n",
    "        \n",
    "        #getting model 3D CNN\n",
    "        #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, min_delta=0.0001)\n",
    "        print(\"Starting VGG 16 3D-----------------------------------------------------\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "\n",
    "        #transfer learning\n",
    "        layers = [l for l in CNN.layers]\n",
    "            \n",
    "        for i in range(len(baseModel.layers)):\n",
    "            layers[i].set_weights(baseModel.layers[i].get_weights())\n",
    "            \n",
    "        \"\"\"\n",
    "        lr_schedule = ExponentialDecay(wandb.config.initial_learning_rate,\n",
    "                                    decay_steps=int((wandb.config.vols_per_session_tr/wandb.config.sub_batch)*len(X_train)),\n",
    "                                    decay_rate=wandb.config.lr_decay_rate, staircase=True)\n",
    "        \"\"\"\n",
    "        CNN.compile(loss=\"categorical_crossentropy\", optimizer=tf.keras.optimizers.Adam(learning_rate=wandb.config.initial_learning_rate),\n",
    "                        metrics=[\"Accuracy\"])\n",
    "\n",
    "        checkpoint_filepath = os.getcwd()+\"/\"+wandb.run.name\n",
    "        #'/tmp/ckpt/MalevsFemale(CPH)_3D-VGG16_flips/'+wandb.run.name\n",
    "        \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='val_loss', mode='min',\n",
    "                                                                save_best_only=True)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"Training\")\n",
    "        \n",
    "        history = CNN.fit(traingen, epochs=wandb.config.epochs, validation_data = valgen, shuffle=True,\n",
    "                        callbacks=[WandbCallback(monitor='val_loss',mode=\"min\",save_model=(False)),model_checkpoint_callback])\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        print('Duration (CNN): {}'.format(end_time - start_time))\n",
    "\n",
    "        y_test=[]\n",
    "        x_vols = []\n",
    "        for i in range(int(len(X_test)*(wandb.config.vols_per_session_ts/wandb.config.sub_batch_ts))):\n",
    "            x,y = testgen[i]\n",
    "            y_test.extend(y)\n",
    "            x_vols.extend(x)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        \n",
    "        print(\"predicts CNN\")\n",
    "        preds = tf.cast(tf.argmax(CNN.predict(testgen), axis=1), tf.int32)\n",
    "\n",
    "        #Wrong predicted subjects\n",
    "        wrong_labeled_subj = mislabeled_subj(y_test, preds, X_test, wandb.config.vols_per_session_ts)\n",
    "\n",
    "        print(\"mislabeled subjects:\\n\",wrong_labeled_subj)\n",
    "        \n",
    "        print(\"evaluating CNN\")\n",
    "        _,acc = CNN.evaluate(testgen, verbose=1)\n",
    "        \n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "\n",
    "        #GradCam\n",
    "        all_layers = [layer.name for layer in reversed(CNN.layers) if len(layer.output_shape) == 5 and (layer.__class__.__name__ == 'ReLU' or isinstance(layer, tf.keras.layers.Conv3D))]\n",
    "        \n",
    "        index_male = index_for_gradcam(0,y_test,preds)\n",
    "        index_female = index_for_gradcam(1,y_test,preds)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        if index_male is None:\n",
    "            index_male=0\n",
    "            print(\"No se encontró ningún sujeto male correctamente clasificado\")\n",
    "            \n",
    "\n",
    "        rabies_crop= rabies_vol[3:45, 4:69, 7:36] #original 48,81,48 #cropped 42,65,29\n",
    "\n",
    "        heatmap_male = make_gradcam_heatmap(np.expand_dims(x_vols[index_male], axis=0), CNN, all_layers[0])\n",
    "        resized_heatmap_male = get_resized_heatmap(heatmap_male,rabies_crop.shape[:3])\n",
    "        resized_heatmap_male= (filter_heatmap(resized_heatmap_male, threshold=0.7))\n",
    "        gradcam_male = create_animation(rabies_crop, 'Male', heatmap=resized_heatmap_male)\n",
    "        name_ani_male = \"GradCam_male(malevsfemale-CPH)-\"+wandb.run.name+\".mp4\"\n",
    "        gradcam_male.save(name_ani_male, writer=writer)\n",
    "\n",
    "        #GradCam average all conv layers\n",
    "        print(\"GradCam All ConvLayers-male\")\n",
    "\n",
    "        all_layers_gradcam_male = fuse_layers(all_layers, CNN, [x_vols[index_male]], 0, emphasize=False)\n",
    "        all_layers_animation_male = create_animation(rabies_crop, 'All_layers_GradCam_male', heatmap=all_layers_gradcam_male)\n",
    "        name_all_male = 'All_layers_GradCam_male'+wandb.run.name+'.mp4'\n",
    "        all_layers_animation_male.save(name_all_male, writer=writer)\n",
    "\n",
    "        #males\n",
    "        if not os.path.exists(os.getcwd()+\"/\"+wandb.run.name+\"/male\"): \n",
    "            # if the demo_folder directory is not present  \n",
    "            # then create it. \n",
    "            os.makedirs(os.getcwd()+\"/\"+wandb.run.name+\"/male\")\n",
    "\n",
    "        \n",
    "        full_grad_male = np.zeros_like((rabies_vol))\n",
    "        full_grad_male[3:45,4:69,7:36] = resized_heatmap_male\n",
    "    \n",
    "\n",
    "        for i in range(rabies_vol.shape[1]):\n",
    "            plt.imshow(cv2.resize(np.rot90(rabies_vol[:,i,:]),dsize=(126,87)), alpha=0.8, cmap='bone')\n",
    "            # over the cam output\n",
    "            plt.imshow(cv2.resize(np.rot90(full_grad_male[:,i,:]),dsize=(126,87)), alpha=0.4,cmap='Reds')\n",
    "            plt.axis('off')\n",
    "            # display the image\n",
    "            name = str(i)+\".png\" \n",
    "            plt.savefig(os.getcwd()+\"/\"+wandb.run.name+\"/male/\"+name)\n",
    "            plt.show()\n",
    "    \n",
    "    \n",
    "        np.save(os.getcwd()+\"/\"+wandb.run.name+\"/male/Array_GradCam-M\", full_grad_male)\n",
    "        \n",
    "        wandb.log({'GradCam_Male-coronal': wandb.Video(name_ani_male),\n",
    "            'GradCam_Male_all-layers': wandb.Video(name_all_male),\n",
    "            'GradCam-per_frames-male': wandb.Image(grad_cam_per_frames(rabies_crop,resized_heatmap_male, 150))})\n",
    "\n",
    "        if index_female is None:\n",
    "            index_female=0\n",
    "            print(\"No se encontró ningún sujeto female correctamente clasificado\")\n",
    "            \n",
    "        heatmap_female = make_gradcam_heatmap(np.expand_dims(x_vols[index_female], axis=0), CNN, all_layers[0]) \n",
    "        resized_heatmap_female = get_resized_heatmap(heatmap_female, rabies_crop.shape[:3])\n",
    "        resized_heatmap_female= (filter_heatmap(resized_heatmap_female, threshold=0.7))\n",
    "        gradcam_female = create_animation(rabies_crop, 'Female', heatmap=resized_heatmap_female)\n",
    "        name_ani_female = \"GradCam_female(malevsfemale-CPH)-\"+wandb.run.name+\".mp4\"\n",
    "        gradcam_female.save(name_ani_female, writer=writer)\n",
    "    \n",
    "        #GradCam average all conv layers\n",
    "        print(\"GradCam All ConvLayers-female\")\n",
    "    \n",
    "        all_layers_gradcam_female = fuse_layers(all_layers, CNN, [x_vols[index_female]], 0, emphasize=False)\n",
    "        all_layers_animation_female = create_animation(rabies_crop, 'All_layers_GradCam_female', heatmap=all_layers_gradcam_female)\n",
    "        name_all_female = 'All_layers_GradCam_female'+wandb.run.name+'.mp4'\n",
    "        all_layers_animation_female.save(name_all_female, writer=writer)\n",
    "    \n",
    "        #Save frame by frame of gradcam\n",
    "        #Females\n",
    "        if not os.path.exists(os.getcwd()+\"/\"+wandb.run.name+\"/female\"): \n",
    "            # if the demo_folder directory is not present  \n",
    "            # then create it. \n",
    "            os.makedirs(os.getcwd()+\"/\"+wandb.run.name+\"/female\")\n",
    "\n",
    "        \n",
    "        #get gradCams with the shape of the original volumes before crop\n",
    "        full_grad_female = np.zeros_like((rabies_vol))\n",
    "        full_grad_female[3:45,4:69,7:36] = resized_heatmap_female\n",
    "        \n",
    "        \n",
    "        for i in range(rabies_vol.shape[1]):\n",
    "            plt.imshow(cv2.resize(np.rot90(rabies_vol[:,i,:]),dsize=(126,87)), alpha=0.8, cmap='bone')\n",
    "            # over the cam output\n",
    "            plt.imshow(cv2.resize(np.rot90(full_grad_female[:,i,:]),dsize=(126,87)), alpha=0.4,cmap='Reds')\n",
    "            plt.axis('off')\n",
    "            # display the image\n",
    "            \n",
    "            name = str(i)+\".png\" \n",
    "            plt.savefig(os.getcwd()+\"/\"+wandb.run.name+\"/female/\"+name)\n",
    "            plt.show()\n",
    "    \n",
    "\n",
    "        np.save(os.getcwd()+\"/\"+wandb.run.name+\"/female/Array_GradCam-F\", full_grad_female)\n",
    "        wandb.log({'GradCam-Female-coronal': wandb.Video(name_ani_female),\n",
    "            'GradCam-Female_all-layers': wandb.Video(name_all_female),\n",
    "            'GradCam-per_frames-female (crop)': wandb.Image(grad_cam_per_frames(rabies_crop, resized_heatmap_female, 150))})#vista recortadfs\n",
    "     \n",
    "     \n",
    "        \n",
    "\n",
    "        print(\"CM CNN\")\n",
    "        cm = confusionmatrix(y_test, preds)\n",
    "\n",
    "        wandb.log({'test_acc': float(acc),\n",
    "                'time_running': '{}'.format(end_time - start_time),\n",
    "                'confution_matrix': wandb.Image(cm),\n",
    "                'mislabeled_subj':wrong_labeled_subj})\n",
    "        \n",
    "\n",
    "        run = run + 1\n",
    "            \n",
    "    print(\"histories and scores from VGG 16 M2D\") \n",
    "    summarize_diagnostics(histories)\n",
    "    summarize_performance(scores)\n",
    "\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nipd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
