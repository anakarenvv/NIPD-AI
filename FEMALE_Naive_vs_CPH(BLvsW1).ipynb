{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8a2d530-3ec9-42fd-be5f-e5efd35b0fa7",
   "metadata": {},
   "source": [
    "# Female Naive Vs Female CPH\n",
    "1. Female CPH (baseline)\n",
    "2. Female CPH (Week 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106ffb1b-7a21-4433-b76f-276108f014de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"C:/Users/PC-EIAD209/Desktop/AnaKei/NIPD-AI\")\n",
    "\n",
    "#sys.path.append(\"C:\\\\Users\\\\\"+os.getlogin()+\"\\\\OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey\\\\PainClassifier\")\n",
    "from my_data_generator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748e72bd-7f6a-4a51-9d62-8c9891680af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import h5py\n",
    "import warnings\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.metrics import precision_recall_fscore_support as score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "import scipy as sp\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import auc, roc_curve\n",
    "from itertools import cycle\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Conv3D, MaxPooling3D, Flatten, Dropout, GlobalAveragePooling3D, concatenate, BatchNormalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils import resample\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from keras.regularizers import l2\n",
    "import cv2\n",
    "from keras import initializers\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.layers import BatchNormalization\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import wandb\n",
    "from wandb.integration.keras import WandbCallback\n",
    "import gc\n",
    "#from numba import cuda\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca2d53-8010-4075-a5e1-55782abcec4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(\"Devices:\", tf.config.list_physical_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25446ae9-81d1-4ee5-a514-bd2439a09094",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cc49e-1a2f-4bc1-a9d0-f96b762bb778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VGG16_3D(blocks):\n",
    "        \n",
    "    inputs = Input(shape=(42, 65, 29), name='input_layer')\n",
    "    x = Reshape(target_shape=[42, 65, 29, 1], name='input_x_3d_volumes')(inputs)\n",
    "\n",
    "    if blocks == 1:\n",
    "        print(\"entra al 1\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 2:\n",
    "        print(\"entra al 2\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 3:\n",
    "        print(\"entra al 3\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "        \n",
    "    elif blocks == 4:\n",
    "        print(\"entra al 4\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x) \n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.2)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer='l2')(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        #x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "    elif blocks == 5:\n",
    "        print(\"entra al 5\")\n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 1st Conv Block\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D(filters =64, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "            \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 2nd Conv Block\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =128, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 3rd Conv block  \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x) \n",
    "        x = Conv3D (filters =256, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x) \n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "        \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 4th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =1, padding ='same')(x)\n",
    "        x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    \n",
    "        #batch_norm\n",
    "        x = BatchNormalization()(x)\n",
    "        # 5th Conv block\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        x = Conv3D (filters =512, kernel_size =3, padding ='same', activation='relu',kernel_regularizer=tf.keras.regularizers.L2(l2=0.05))(x)\n",
    "        #x = MaxPooling3D(pool_size =2, strides =2, padding ='same')(x)\n",
    "        x = tf.keras.layers.GlobalAveragePooling3D()(x)\n",
    "        x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    \n",
    "    # Fully connected layers  \n",
    "    x = Flatten()(x) \n",
    "    x = Dense(units = 4096, activation ='relu',kernel_regularizer='l2')(x)\n",
    "    #x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    x = Dense(units = 4096, activation ='relu',kernel_regularizer='l2')(x) \n",
    "    x = tf.keras.layers.Dropout(0.5)(x)\n",
    "    output = Dense(units = 2,activation ='sigmoid',kernel_regularizer='l2')(x)\n",
    "    \n",
    "    # creating the model\n",
    "    VGG_3d_model = Model (inputs=inputs, outputs =output)\n",
    "    #model.summary()\n",
    "\n",
    "    return VGG_3d_model\n",
    "\n",
    "def set_pretrained_weigths(VGG_3d_model):\n",
    "    #VGG 16 with weights from Imagenet\n",
    "    pretrained_model = tf.keras.applications.VGG16(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling='avg',\n",
    "        input_shape = (42, 65, 3)\n",
    "    )\n",
    "    \n",
    "    #conv layers on VGG_3d_model\n",
    "    layers_conv = []\n",
    "    for j in range(len(VGG_3d_model.layers)):\n",
    "        if \"conv3d\" in str(VGG_3d_model.layers[j]):\n",
    "            layers_conv.append(j)\n",
    "    layers_conv_pretrained = []\n",
    "    for j in range(len(pretrained_model.layers)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[j]):\n",
    "            layers_conv_pretrained.append(j)\n",
    "    \n",
    "    for i in range(len(layers_conv)):\n",
    "        if \"Conv2D\" in str(pretrained_model.layers[layers_conv_pretrained[i]]):\n",
    "            if i == 0:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0].sum(axis=2, keepdims=True)\n",
    "            else:\n",
    "                w = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[0]\n",
    "                \n",
    "            w3d=[]\n",
    "            \n",
    "            w = np.reshape(w,(3,3,-1),order='F')\n",
    "            for j in range(len(w[0,0,:])):\n",
    "                for k in range(3):\n",
    "                    w3d.append(w[:,:,j])\n",
    "            w3d = np.transpose(w3d, (1,2,0))\n",
    "            \n",
    "            new_weights = np.reshape(w3d, np.array(VGG_3d_model.layers[layers_conv[i]].get_weights()[0]).shape,order='F')\n",
    "            new_bias = pretrained_model.layers[layers_conv_pretrained[i]].get_weights()[1]\n",
    "            \n",
    "            WnB = []\n",
    "            WnB.append(new_weights)\n",
    "            WnB.append(new_bias)\n",
    "    \n",
    "            VGG_3d_model.layers[layers_conv[i]].set_weights(WnB)\n",
    "\n",
    "    del pretrained_model, w, WnB, new_weights, new_bias, w3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b73eef-388d-4077-81f8-6eb93c39f318",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732562b4-13a5-447d-8802-6be04933fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusionmatrix_multiclass(y_test,pred):\n",
    "    cm = confusion_matrix(y_test, (np.rint(preds)).astype(int) )\n",
    "    group_names = ['True baseline','False Baseline','False Baseline',   \n",
    "                   'False week 1','Truec','False Week 1',\n",
    "                  'False week 7','False week 7','True week 7']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(3,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(3,3)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['Baseline','Week 1','Week 7'] ,yticklabels = ['Baseline','Week 1','Week 7'])\n",
    "    plt.show()\n",
    "\n",
    "def confusionmatrix(y_test,preds):\n",
    "    #Construct the Confusion Matrix\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True Naive','False Naive','False CPH','True CPH']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['MALE','FEMALE'] ,yticklabels = ['MALE','FEMALE'])\n",
    "    plt.show()\n",
    "    return sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['MALE','FEMALE'] ,yticklabels = ['MALE','FEMALE'])\n",
    "    \n",
    "def confusionmatrix_binary(y_test, preds):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    group_names = ['True baseline','False baseline','False Week 1','True Week 1']\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                    cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         np.ndarray.flatten(cm/(np.sum(cm,axis=1).reshape(2,1)))]\n",
    "    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "              zip(group_names,group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(cm, annot=labels, fmt='', cmap='Blues', xticklabels = ['MALE','FEMALE'] ,yticklabels = ['MALE','FEMALE'])\n",
    "    plt.show()\n",
    "\n",
    "def ROC(probs,y_test): #binary\n",
    "    #Classification Area under curve\n",
    "     warnings.filterwarnings('ignore')\n",
    "             \n",
    "     auc = roc_auc_score(y_test, probs)\n",
    "     print('AUC - Test Set: %.2f%%' % (auc*100))\n",
    "    \n",
    "     # calculate roc curve\n",
    "     fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "     # plot no skill\n",
    "     plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "     # plot the roc curve for the model\n",
    "     plt.plot(fpr, tpr, marker='.')\n",
    "     plt.xlabel('False positive rate')\n",
    "     plt.ylabel('Sensitivity/ Recall')\n",
    "     # show the plot\n",
    "     plt.show()\n",
    "    \n",
    "     probs = (np.rint(probs)).astype(int)   \n",
    "        \n",
    "     precision = precision_score(y_test, probs)\n",
    "     print('Precision: %f' % precision)\n",
    "     # recall: tp / (tp + fn)\n",
    "     recall = recall_score(y_test, probs)\n",
    "     print('Recall: %f' % recall)\n",
    "     # f1: tp / (tp + fp + fn)\n",
    "     f1 = f1_score(y_test, probs)\n",
    "     print('F1 score: %f' % f1)\n",
    "        \n",
    "def ROC_multiclass(model, y_test, n_class):\n",
    "    #y_test: array size (# of subjects, ) with classes \n",
    "    #pretrained model to be evaluated \n",
    "    \n",
    "    label_binarizer = LabelBinarizer().fit(y_test)\n",
    "    y_onehot_test = label_binarizer.transform(y_test)\n",
    "    y_onehot_test.shape  # (n_samples, n_classes)\n",
    "\n",
    "    y_score = model.predict(X_test) # y_score is onehot\n",
    "    \n",
    "    # store the fpr, tpr, and roc_auc for all averaging strategies\n",
    "    fpr, tpr, roc_auc = dict(), dict(), dict()\n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_onehot_test.ravel(), y_score.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    print(f\"Micro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['micro']:.2f}\")\n",
    "    \n",
    "    n_classes = n_class\n",
    "    \n",
    "    for i in range(n_classes):\n",
    "        fpr[i], tpr[i], _ = roc_curve(y_onehot_test[:, i], y_score[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "    fpr_grid = np.linspace(0.0, 1.0, 1000)\n",
    "\n",
    "    # Interpolate all ROC curves at these points\n",
    "    mean_tpr = np.zeros_like(fpr_grid)\n",
    "\n",
    "    for i in range(n_classes):\n",
    "        mean_tpr += np.interp(fpr_grid, fpr[i], tpr[i])  # linear interpolation\n",
    "\n",
    "    # Average it and compute AUC\n",
    "    mean_tpr /= n_classes\n",
    "\n",
    "    fpr[\"macro\"] = fpr_grid\n",
    "    tpr[\"macro\"] = mean_tpr\n",
    "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
    "\n",
    "    print(f\"Macro-averaged One-vs-Rest ROC AUC score:\\n{roc_auc['macro']:.2f}\")\n",
    "    \n",
    "    target_names = ['Naive','Week1','Week7']\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"micro\"],\n",
    "        tpr[\"micro\"],\n",
    "        label=f\"micro-average ROC curve (AUC = {roc_auc['micro']:.2f})\",\n",
    "        color=\"deeppink\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    plt.plot(\n",
    "        fpr[\"macro\"],\n",
    "        tpr[\"macro\"],\n",
    "        label=f\"macro-average ROC curve (AUC = {roc_auc['macro']:.2f})\",\n",
    "        color=\"navy\",\n",
    "        linestyle=\":\",\n",
    "        linewidth=4,\n",
    "    )\n",
    "\n",
    "    colors = cycle([\"aqua\", \"darkorange\", \"cornflowerblue\"])\n",
    "    for class_id, color in zip(range(n_classes), colors):\n",
    "        RocCurveDisplay.from_predictions(\n",
    "            y_onehot_test[:, class_id],\n",
    "            y_score[:, class_id],\n",
    "            name=f\"ROC curve for {target_names[class_id]}\",\n",
    "            color=color,\n",
    "            ax=ax,\n",
    "            plot_chance_level=(class_id == 2),\n",
    "        )\n",
    "\n",
    "    _ = ax.set(\n",
    "        xlabel=\"False Positive Rate\",\n",
    "        ylabel=\"True Positive Rate\",\n",
    "        title=\"Extension of Receiver Operating Characteristic\\nto One-vs-Rest multiclass\",\n",
    "    )\n",
    "    \n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(histories):\n",
    "    c = ['b','g','r','c','m','y','k','w']\n",
    "    ltr = ['fold 1(train)','fold 2(train)','fold 3(train)','fold 4(train)','fold 5(train)']\n",
    "    lts = ['fold 1(val)','fold 2(val)','fold 3(val)','fold 4(val)','fold 5(val)']\n",
    "    for i in range(len(histories)):\n",
    "        # plot loss\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.title('Cross Entropy Loss')\n",
    "        plt.plot(histories[i].history['loss'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_loss'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "        # plot accuracy\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.title('Classification Accuracy')\n",
    "        plt.plot(histories[i].history['Accuracy'], color=c[i], label=ltr[i], linestyle=\"-\")\n",
    "        plt.plot(histories[i].history['val_Accuracy'], color=c[i], label=lts[i], linestyle=\"--\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# summarize model performance\n",
    "def summarize_performance(scores):\n",
    "    # print summary\n",
    "    print('Accuracy: mean=%.3f std=%.3f, n=%d' % (np.mean(scores)*100, np.std(scores)*100, len(scores)))\n",
    "    # box and whisker plots of results\n",
    "    plt.boxplot(scores)\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08627ebe-a7e5-4c2c-a81f-5c3fd1f67025",
   "metadata": {},
   "source": [
    "# Just brain. Female. Naive vs CPH\n",
    "1) Naive (CPH_BL)\n",
    "2) CPH (CPH_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31333e60-218e-4b1a-bd56-598d92c0dc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#female = [49,50,51,52,65,66,77,78,79,80,81,82,83]\n",
    "#female = [49,50,51,52,65,66]\n",
    "female=[49,50,51,52,65,66,77,78,79]\n",
    "y_female = np.ones(len(female))\n",
    "\n",
    "subjects = np.array(female)\n",
    "labels = np.array(list(y_female))\n",
    "sessions = [1,2]\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"rest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c966a-ca55-4ed6-a631-775f18290343",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5861269-6b1d-4a43-a0cd-abc4f3adbad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 80% train y val y 20% test\n",
    "sub_trainval, sub_test, y_trainval, y_test = train_test_split(\n",
    "    subjects, labels, test_size=0.2, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "n_bootstraps= 2\n",
    "for boot in range(n_bootstraps): \n",
    "    print(f\"\\n=== Bootstrapping Run {boot+1}/{n_bootstraps} ===\")\n",
    "\n",
    "    # Resample \n",
    "    #boot_subjects, boot_labels = resample(subjects, labels, replace=True, random_state=boot)\n",
    "    boot_subjects, boot_labels = resample(\n",
    "    sub_trainval, y_trainval, replace=True, random_state=42+boot\n",
    "    )\n",
    "    \n",
    "    kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "\n",
    "    scores, histories = list(), list()\n",
    "    run = 1\n",
    "    for train_ix, val_ix in kfold.split(sub_trainval, y_trainval):\n",
    "        print(\"Run #\",run)\n",
    "\n",
    "        #sub_train,sub_val = sub_trainval[list(train_ix)], sub_trainval[list(val_ix)]\n",
    "        #y_train, y_val = y_trainval[list(train_ix)], y_trainval[list(val_ix)]\n",
    "\n",
    "        sub_train, sub_val = boot_subjects[train_ix], boot_subjects[val_ix]\n",
    "        y_train, y_val     = boot_labels[train_ix], boot_labels[val_ix]\n",
    "        \n",
    "        # Set default values\n",
    "        config_defaults = {\n",
    "            \"batch\": 1,\n",
    "        }\n",
    "        \n",
    "        # Initialize wandb with a sample project name\n",
    "        wandb.init(project=\"FEMALE_Naive_vs_CPH(BLvsW1)\", notes=\"No pooling. New data augmentation. Z-scoring per batch. Batch = 4. 10 epoch. Just z-scoring and applying RandomFlip. All layers gradcam. lr = 1e-5\",\n",
    "                    config=config_defaults)\n",
    "\n",
    "        # Specify the other hyperparameters to the configuration.\n",
    "        wandb.config.epochs = 2\n",
    "        wandb.config.sub_batch = 30\n",
    "        wandb.config.sub_batch_ts = 30\n",
    "        wandb.config.subjects = subjects\n",
    "        wandb.config.architecture_name = \"VGG16_3D\"\n",
    "        wandb.config.dataset_name = \"NAIVE vs CPH(CPH[Bl-W1])\"\n",
    "        wandb.config.CNN_blocks = 5\n",
    "        wandb.config.sessions = sessions\n",
    "        wandb.config.vols_per_session_tr = 30 #570\n",
    "        wandb.config.vols_per_session_ts = 30 #570\n",
    "        wandb.config.initial_learning_rate = 1e-5\n",
    "        #wandb.config.lr_decay_rate = 0.95\n",
    "        wandb.config.optimizer = \"Adam\"\n",
    "        \n",
    "        '''\n",
    "        sub_train = subjects[list(train_ix)]\n",
    "        y_tr_all = labels[list(train_ix)]\n",
    "        \n",
    "        val_size = max(2, int(round(0.1 * len(sub_train))))\n",
    "        \n",
    "        sub_train, sub_val, y_train, y_val  = train_test_split(sub_train, labels[list(train_ix)], test_size=val_size, random_state=42, \n",
    "                                                            stratify=y_tr_all)\n",
    "        sub_test = subjects[list(val_ix)]\n",
    "        '''\n",
    "\n",
    "        val_size = max(2, int(round(0.1 * len(train_ix))))\n",
    "    \n",
    "        sub_train, sub_val, y_train, y_val = train_test_split(\n",
    "            sub_trainval[train_ix], y_trainval[train_ix],\n",
    "            test_size=val_size,  # test_size se refiere a la validacion\n",
    "            random_state=42, stratify=y_trainval[train_ix]\n",
    "        )\n",
    "\n",
    "        sub_test, y_test = sub_test, y_test\n",
    "        \n",
    "        CPHclassTrain = FILES_and_LABELS(sub_train, sessions, MRI_type, functional_type)\n",
    "        CPHclassTest = FILES_and_LABELS(sub_test, sessions, MRI_type, functional_type)\n",
    "        CPHclassval = FILES_and_LABELS(sub_val, sessions, MRI_type, functional_type)\n",
    "            \n",
    "        X_train = CPHclassTrain.get_mask_and_bold()\n",
    "        X_test = CPHclassTest.get_mask_and_bold()\n",
    "        X_val = CPHclassval.get_mask_and_bold()\n",
    "\n",
    "        wandb.config.batch = 4\n",
    "\n",
    "        print(\"sub train:\")\n",
    "        print(np.array(X_train)[:,0])\n",
    "        print(\"sub test:\")\n",
    "        print(np.array(X_test)[:,0])\n",
    "        print(\"sub val:\")\n",
    "        print(np.array(X_val)[:,0])\n",
    "\n",
    "        print(\"# sesiones Train\",len(X_train))\n",
    "        print(\"# sesiones Test\",len(X_test))\n",
    "        print(\"# sesiones Val\",len(X_val))\n",
    "        \n",
    "        traingen = CustomDataGen(X_train, batch_size=wandb.config.batch, subbatch_size=wandb.config.sub_batch,\n",
    "                                    format = \"just_brain\", vols = wandb.config.vols_per_session_tr,\n",
    "                                    num_class = 2, classes = \"CPHvsNAIVEfemale\", augmentation = True)\n",
    "        traingen.on_epoch_end()\n",
    "        #Es necesario que la division entre X_test y batch_size tenga un modulo igual a 0. \n",
    "        #De otra manera el ultimo batch no lo utiliza al utilizar .predict\n",
    "        testgen  = CustomDataGen(X_test, batch_size=1,subbatch_size=wandb.config.sub_batch_ts,\n",
    "                                    format = \"just_brain\", vols= wandb.config.vols_per_session_ts,\n",
    "                                    num_class = 2, classes = \"CPHvsNAIVEfemale\",shuffle=False)\n",
    "        valgen  = CustomDataGen(X_val, batch_size=len(X_val),subbatch_size=30, format = \"just_brain\",vols=570, num_class = 2, classes = \"CPHvsNAIVEfemale\")\n",
    "        \n",
    "        #getting model 3D CNN\n",
    "        #callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, min_delta=0.0001)\n",
    "        print(\"Starting VGG 16 3D-----------------------------------------------------\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        gc.collect()\n",
    "        CNN = VGG16_3D(3)\n",
    "        set_pretrained_weigths(CNN)\n",
    "\n",
    "        \"\"\"\n",
    "        lr_schedule = ExponentialDecay(wandb.config.initial_learning_rate,\n",
    "                                        decay_steps=int((wandb.config.vols_per_session_tr/wandb.config.sub_batch)*len(X_train)),\n",
    "                                        decay_rate=wandb.config.lr_decay_rate, staircase=True)\n",
    "        \"\"\"\n",
    "        CNN.compile(loss=tf.nn.softmax_cross_entropy_with_logits, optimizer=tf.keras.optimizers.Adam(learning_rate=wandb.config.initial_learning_rate),\n",
    "                        metrics=[\"Accuracy\"])\n",
    "\n",
    "        checkpoint_filepath = os.getcwd()+\"/\"+wandb.run.name\n",
    "        #'/tmp/ckpt/MalevsFemale(CPH)_3D-VGG16_flips/'+wandb.run.name\n",
    "\n",
    "        acc_loss_rate = CombineCallback()\n",
    "        \n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath, monitor='combine_metric', mode='max',\n",
    "                                                                save_best_only=True)\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"Training\")\n",
    "        \n",
    "        history = CNN.fit(traingen, epochs=wandb.config.epochs, validation_data = valgen, shuffle=True,\n",
    "                            callbacks=[WandbCallback(monitor='combine_metric',mode=\"max\",save_model=(False)),acc_loss_rate,model_checkpoint_callback])\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        print('Duration (CNN): {}'.format(end_time - start_time))\n",
    "\n",
    "        print(\"Evaluating best epoch\")\n",
    "        CNN.load_weights(checkpoint_filepath)\n",
    "        CNN.compile(loss=tf.nn.softmax_cross_entropy_with_logits, optimizer=tf.keras.optimizers.Adam(learning_rate=wandb.config.initial_learning_rate),\n",
    "                        metrics=[\"Accuracy\"])\n",
    "        _,acc = CNN.evaluate(testgen, verbose=1)\n",
    "\n",
    "        y_test=[]\n",
    "        x_vols = []\n",
    "        for i in range(int(len(X_test)*(wandb.config.vols_per_session_ts/wandb.config.sub_batch_ts))):\n",
    "            x,y = testgen[i]\n",
    "            y_test.extend(y)\n",
    "            x_vols.extend(x)\n",
    "        y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "        \n",
    "        print(\"predicts CNN\")\n",
    "        preds = tf.cast(tf.argmax(CNN.predict(testgen), axis=1), tf.int32)\n",
    "\n",
    "        #Wrong predicted subjects\n",
    "        wrong_labeled_subj = mislabeled_subj(y_test, preds, X_test, wandb.config.vols_per_session_ts)\n",
    "\n",
    "        print(\"mislabeled subjects:\\n\",wrong_labeled_subj)\n",
    "        \n",
    "        scores.append(acc)\n",
    "        histories.append(history)\n",
    "\n",
    "        #GradCam\n",
    "        #Naive\n",
    "        if not os.path.exists(os.getcwd()+\"/\"+wandb.run.name+\"/Naive\"): \n",
    "            # if the demo_folder directory is not present  \n",
    "            # then create it. \n",
    "            os.makedirs(os.getcwd()+\"/\"+wandb.run.name+\"/Naive\")\n",
    "        #CPH\n",
    "        if not os.path.exists(os.getcwd()+\"/\"+wandb.run.name+\"/CPH\"): \n",
    "            # if the demo_folder directory is not present  \n",
    "            # then create it. \n",
    "            os.makedirs(os.getcwd()+\"/\"+wandb.run.name+\"/CPH\")\n",
    "\n",
    "        all_layers = [layer.name for layer in reversed(CNN.layers) if len(layer.output_shape) == 5 and (layer.__class__.__name__ == 'ReLU' or isinstance(layer, tf.keras.layers.Conv3D))]\n",
    "        \n",
    "        index_naive = index_for_gradcam(0,y_test,preds)\n",
    "        index_cph = index_for_gradcam(1,y_test,preds)\n",
    "\n",
    "\n",
    "        if index_naive is None:\n",
    "            index_naive = 0\n",
    "\n",
    "            print(\"No se encontró ningún sujeto Naive correctamente clasificado\")\n",
    "            print(\"index_naive se pone en 0 SOLO para poder correr el GradCAM\")\n",
    "\n",
    "\n",
    "        if index_cph is None:\n",
    "            print(\"No se encontró ningún sujeto CPH correctamente clasificado\")\n",
    "            print(\"index_cph se pone en 0 SOLO para poder correr el GradCAM\")\n",
    "            index_cph = 0\n",
    "\n",
    "        \n",
    "        #if index_naive is not None and index_cph is not None:\n",
    "\n",
    "        # GradCAM por clase\n",
    "        heatmap_naive = make_gradcam_heatmap(np.expand_dims(x_vols[index_naive], axis=0), CNN, all_layers[0])\n",
    "        heatmap_cph   = make_gradcam_heatmap(np.expand_dims(x_vols[index_cph],   axis=0), CNN, all_layers[0])\n",
    "\n",
    "        resized_heatmap_naive = get_resized_heatmap(heatmap_naive, np.shape(x_vols[index_naive]))\n",
    "        resized_heatmap_cph   = get_resized_heatmap(heatmap_cph,   np.shape(x_vols[index_cph]))\n",
    "\n",
    "        # Animaciones coronal\n",
    "        gradcam_naive = create_animation(x_vols[index_naive], 'Naive', heatmap=resized_heatmap_naive)\n",
    "        gradcam_cph   = create_animation(x_vols[index_cph],   'CPH',   heatmap=resized_heatmap_cph)\n",
    "\n",
    "        Writer = animation.writers['ffmpeg']\n",
    "        writer = Writer(fps=15, metadata=dict(artist='Me'), bitrate=1800)\n",
    "\n",
    "        name_ani_naive = os.getcwd() + \"/\" + wandb.run.name + \"/Naive/GradCam_Naive(BlVsW1-CPH).mp4\"\n",
    "        gradcam_naive.save(name_ani_naive, writer=writer)\n",
    "\n",
    "        name_ani_CPH = os.getcwd() + \"/\" + wandb.run.name + \"/CPH/GradCam_CPH(BlVsW1-CPH).mp4\"\n",
    "        gradcam_cph.save(name_ani_CPH, writer=writer)\n",
    "\n",
    "        # GradCAM promedio de todas las capas convolucionales\n",
    "        print(\"GradCam All ConvLayers\")\n",
    "        all_layers_gradcam_naive = fuse_layers(all_layers, CNN, x_vols, index_naive, emphasize=False)\n",
    "        all_layers_gradcam_cph   = fuse_layers(all_layers, CNN, x_vols, index_cph,   emphasize=False)\n",
    "\n",
    "        all_layers_animation_naive = create_animation(x_vols[index_naive], 'all_layers_gradcam_Naive', heatmap=all_layers_gradcam_naive)\n",
    "        all_layers_animation_cph   = create_animation(x_vols[index_cph],   'all_layers_gradcam_CPH',   heatmap=all_layers_gradcam_cph)\n",
    "\n",
    "        name_all_naive = os.getcwd() + \"/\" + wandb.run.name + \"/Naive/all_layers_gradcam_Naive.mp4\"\n",
    "        all_layers_animation_naive.save(name_all_naive, writer=writer)\n",
    "\n",
    "        name_all_cph = os.getcwd() + \"/\" + wandb.run.name + \"/CPH/all_layers_gradcam_CPH.mp4\"\n",
    "        all_layers_animation_cph.save(name_all_cph, writer=writer)\n",
    "\n",
    "        # Guardar PNGs frame por frame\n",
    "        for i in range(len(x_vols[index_cph][0,:,0])):\n",
    "            plt.imshow(cv2.resize(np.rot90(np.array(x_vols[index_cph])[:,i,:]), dsize=(126,87)), alpha=0.8, cmap='bone')\n",
    "            plt.imshow(cv2.resize(np.rot90(resized_heatmap_cph[:,i,:]), dsize=(126,87)), alpha=0.4, cmap='jet')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.getcwd()+\"/\"+wandb.run.name+\"/CPH/\"+str(i)+\".png\")\n",
    "            plt.show()\n",
    "\n",
    "        for i in range(len(x_vols[index_naive][0,:,0])):\n",
    "            plt.imshow(cv2.resize(np.rot90(np.array(x_vols[index_naive])[:,i,:]), dsize=(126,87)), alpha=0.8, cmap='bone')\n",
    "            plt.imshow(cv2.resize(np.rot90(resized_heatmap_naive[:,i,:]), dsize=(126,87)), alpha=0.4, cmap='jet')\n",
    "            plt.axis('off')\n",
    "            plt.savefig(os.getcwd()+\"/\"+wandb.run.name+\"/Naive/\"+str(i)+\".png\")\n",
    "            plt.show()\n",
    "\n",
    "        # Guardar arrays GradCAM\n",
    "        np.save(os.getcwd()+\"/\"+wandb.run.name+\"/CPH/Array_GradCam-CPH\", resized_heatmap_cph)\n",
    "        np.save(os.getcwd()+\"/\"+wandb.run.name+\"/Naive/Array_GradCam-Naive\", resized_heatmap_naive)\n",
    "\n",
    "        print(\"CM CNN\")\n",
    "        cm = confusionmatrix(y_test, preds)\n",
    "\n",
    "        # Log completo en Weights & Biases\n",
    "        wandb.log({\n",
    "            'test_acc': float(acc),\n",
    "            'time_running': '{}'.format(end_time - start_time),\n",
    "            'confution_matrix': wandb.Image(cm),\n",
    "            'mislabeled_subj': wrong_labeled_subj,\n",
    "            'GradCam_Naive-coronal': wandb.Video(name_ani_naive),\n",
    "            'GradCam-CPH-coronal': wandb.Video(name_ani_CPH),\n",
    "            'GradCam_Naive_all-layers': wandb.Video(name_all_naive),\n",
    "            'GradCam-CPH_all-layers': wandb.Video(name_all_cph),\n",
    "            'GradCam-per_frames-Naive': wandb.Image(grad_cam_per_frames(x_vols[index_naive], resized_heatmap_naive, threshold=0.3)),\n",
    "            'GradCam-per_frames-CPH': wandb.Image(grad_cam_per_frames(x_vols[index_cph], resized_heatmap_cph, threshold=0.3))\n",
    "        })\n",
    "\n",
    "\n",
    "        run = run + 1\n",
    "            \n",
    "    print(\"histories and scores from VGG 16 M2D\") \n",
    "    summarize_diagnostics(histories)\n",
    "    summarize_performance(scores)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28ae67f-97ec-4369-8cff-104825d88e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/restful-disco-20/Naive/Array_GradCam-Naive.npy\"\n",
    "path2_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/dazzling-dawn-21/Naive/Array_GradCam-Naive.npy\"\n",
    "path3_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/wandering-surf-22/Naive/Array_GradCam-Naive.npy\"\n",
    "path4_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/amber-brook-23/Naive/Array_GradCam-Naive.npy\"\n",
    "\n",
    "cam1 = np.load(path1_male)\n",
    "cam2 = np.load(path2_male)\n",
    "cam3 = np.load(path3_male)\n",
    "cam4 = np.load(path4_male)\n",
    "\n",
    "avgcam = np.mean(np.array([cam1,cam2,cam3,cam4]),axis=0)\n",
    "\n",
    "print(\"max\",avgcam.max())\n",
    "print(\"min\",avgcam.min())\n",
    "\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"rest\"\n",
    "CPHclassTrain = FILES_and_LABELS([82], [1], MRI_type, functional_type)\n",
    "\n",
    "X_train = CPHclassTrain.get_mask_and_bold()\n",
    "traingen = CustomDataGen(X_train, batch_size=1, subbatch_size=30,\n",
    "                                 format = \"just_brain\", vols = 570,\n",
    "                                 num_class = 2, classes = \"sex\")\n",
    "\n",
    "x,y = traingen[0]\n",
    "\n",
    "grad_cam_per_frames(x[25],avgcam,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bdcfb2-c8c1-4736-9c95-ee024a993ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path1_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/restful-disco-20/CPH/Array_GradCam-CPH.npy\"\n",
    "path2_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/dazzling-dawn-21/CPH/Array_GradCam-CPH.npy\"\n",
    "path3_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/wandering-surf-22/CPH/Array_GradCam-CPH.npy\"\n",
    "path4_male = \"C:/Users/gdaalumno/OneDrive - Instituto Tecnologico y de Estudios Superiores de Monterrey/PainClassifier/Transfer Learning/VGG 16/FEMALE_Naive_vs_CPH(BLvsW1)/amber-brook-23/CPH/Array_GradCam-CPH.npy\"\n",
    "\n",
    "cam1 = np.load(path1_male)\n",
    "cam2 = np.load(path2_male)\n",
    "cam3 = np.load(path3_male)\n",
    "cam4 = np.load(path4_male)\n",
    "\n",
    "avgcam = np.mean(np.array([cam1,cam2,cam3,cam4]),axis=0)\n",
    "\n",
    "print(\"max\",avgcam.max())\n",
    "print(\"min\",avgcam.min())\n",
    "\n",
    "MRI_type = \"func\"\n",
    "functional_type = \"rest\"\n",
    "CPHclassTrain = FILES_and_LABELS([82], [2], MRI_type, functional_type)\n",
    "\n",
    "X_train = CPHclassTrain.get_mask_and_bold()\n",
    "traingen = CustomDataGen(X_train, batch_size=1, subbatch_size=30,\n",
    "                                 format = \"just_brain\", vols = 570,\n",
    "                                 num_class = 2, classes = \"sex\")\n",
    "\n",
    "x,y = traingen[0]\n",
    "\n",
    "grad_cam_per_frames(x[25],avgcam,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e42a2-b8af-41e5-bf7d-0f3f67d300cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nipd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
